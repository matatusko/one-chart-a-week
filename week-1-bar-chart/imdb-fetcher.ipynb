{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a14887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a555e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFetcher():\n",
    "    \"\"\"\n",
    "    Utility class to provide methods that allow fetching data from the web, \n",
    "    logging precedures and converting fetched responses to BeautifulSoup \n",
    "    object for further parsing.\n",
    "\n",
    "    Uses fake-useragent library to rotate headers to avoid being blocked.\n",
    "    \"\"\"\n",
    "    def __init__(self, logging_level='info'):\n",
    "        self._init_logger()\n",
    "        self.set_logger_level(logging_level)\n",
    "            \n",
    "    def _init_logger(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                fmt='%(asctime)s - %(name)s - %(levelname)s.%(funcName)s - %(message)s',\n",
    "                datefmt='%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            \n",
    "    def _request(self, url):\n",
    "        return requests.get(url, headers={'User-Agent': UserAgent().random})\n",
    "        \n",
    "    def _soup(self, r):\n",
    "        return BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    def _format_date(self, date_string, expected_format, desired_format='%Y-%m-%d'):\n",
    "        return datetime.strptime(date_string, expected_format).strftime(desired_format)\n",
    "    \n",
    "    def set_logger_level(self, level:str='info'):\n",
    "        str2level = {\n",
    "            'notset': logging.NOTSET,\n",
    "            'debug': logging.DEBUG,\n",
    "            'info': logging.INFO,\n",
    "            'warning': logging.WARNING,\n",
    "            'error': logging.ERROR,\n",
    "            'critica;': logging.CRITICAL,\n",
    "        }\n",
    "        \n",
    "        self.logger.setLevel(str2level.get(level.lower(), logging.NOTSET))\n",
    "        \n",
    "    def request_and_soup(self, url):\n",
    "        '''\n",
    "        Fires request to given url and converts the content to BeautifulSoup for parsing.\n",
    "        Throw HTTPError if page was not found, to be dealt with on the client side\n",
    "        '''\n",
    "        r = self._request(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            self.logger.debug(f'GET request to {url} successful.')\n",
    "            return self._soup(r)\n",
    "        \n",
    "        self.logger.warning(f'GET request to {url} failed. Status Code: {r.status_code}')\n",
    "        r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead2dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBFetcher(BaseFetcher):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.imdb = 'https://www.imdb.com'\n",
    "        \n",
    "    def by_id(self, id, fetch_episodes=False):\n",
    "        endpoint = f'{self.imdb}/title/{id}/'\n",
    "        soup = self.request_and_soup(endpoint)\n",
    "        \n",
    "        results = {\n",
    "            'title': self._get_title(soup),\n",
    "            'rating': self._get_rating(soup)\n",
    "        }\n",
    "        \n",
    "        if fetch_episodes:\n",
    "            results['episodes'] = self._get_episodes(id)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def _get_title(self, soup):\n",
    "        try:\n",
    "            result = soup.select(\"div[class*='OriginalTitle']\")[0].text.replace('Original title: ', '')\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f'Could not extract original title. Error: {e}')\n",
    "            result = None\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _get_rating(self, soup):\n",
    "        try:\n",
    "            result = float(soup.select(\"div[class*='AggregateRatingButton']\")[1].text.replace('/10', ''))\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f'Could not extract score. Error: {e}')\n",
    "            result = None\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _get_episodes(self, id, results=None, current_season=1, latest_season=None):\n",
    "        if current_season == latest_season:\n",
    "            return results\n",
    "        if not results:\n",
    "            results = []\n",
    "        \n",
    "        endpoint = f'{self.imdb}/title/{id}/episodes?season={current_season}'\n",
    "        soup = self.request_and_soup(endpoint)\n",
    "        \n",
    "        # Get selectable seasons\n",
    "        seasons = []\n",
    "        for option in soup.select(\"select[id='bySeason']\")[0].find_all('option'):\n",
    "            seasons.append(int(option.text.strip()))\n",
    "        \n",
    "        for episode in soup.select('div[class=\"list detail eplist\"] > div'):\n",
    "            results.append({\n",
    "                'season': current_season,\n",
    "                'episode': int(episode.select('meta[content]')[0]['content']),\n",
    "                'airdate': self._format_date(episode.select('div[class=\"airdate\"]')[0].text.strip(), '%d %b. %Y'),\n",
    "                'title': episode.select('strong')[0].text.strip(),\n",
    "                'rating': float(episode.select('span[class*=\"star__rating\"]')[0].text.strip()),\n",
    "                'votes': int(re.sub(r'\\(|\\)|,', '', episode.select('span[class*=\"star__total-votes\"]')[0].text.strip())),\n",
    "                'description': episode.select('div[class=\"item_description\"]')[0].text.strip()\n",
    "            })\n",
    "            \n",
    "        results = self._get_episodes(id, results, current_season+1, seasons[-1])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16991e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = IMDBFetcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc022c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "witcher = fetcher.by_id('tt5180504', fetch_episodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b79ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('witcher-episodes.json', 'w') as f:\n",
    "    json.dump(witcher, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a632a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
